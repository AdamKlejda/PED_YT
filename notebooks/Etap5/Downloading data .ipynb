{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from utils import searchVideosByListOfIds, fillCategoryIds\n",
    "from datetime import datetime, timedelta\n",
    "import sys, traceback\n",
    "\n",
    "api_key = os.environ.get('YT_API')\n",
    "# api_key = \"XYZ\"\n",
    "\n",
    "# df = pd.read_csv(\"../../our_data/Etap4/GB_US-rdy-to-learn-properly-V2.csv\")\n",
    "# df = pd.read_csv(\"../../our_data/Etap3/dfGB_merged.csv\")\n",
    "# df = pd.read_csv(\"../../our_data/Etap1/GB_US.csv\")\n",
    "\n",
    "todownload = {0:\"US\",1:\"GB\"}\n",
    "download = todownload[1] #HERE CHOSE WHICH ONE TO DOWNLOAD\n",
    "NUMBER_OF_VIDEOS = 10\n",
    "\n",
    "start_datetime = \"2017-11-14T23:59:59Z\"\n",
    "end_datetime = \"2018-06-14T23:59:59Z\"\n",
    "\n",
    "\n",
    "COLUMNS =['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
    "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
    "       'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
    "       'video_error_or_removed', 'description ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING LIST OF CHANNEL_TITLES NAMES (RUN IT IF YOU NEED A NEW ONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBdf = pd.read_csv(\"./../../data/youtube_data/GB_videos_5p.csv\", sep=';', encoding='utf_16')\n",
    "# USdf = pd.read_csv(\"./../../data/youtube_data/US_videos_5p.csv\", sep=';')\n",
    "# GB_names = pd.DataFrame(set(GBdf.channel_title))\n",
    "# US_names = pd.DataFrame(set(USdf.channel_title))\n",
    "# GB_names.to_csv(\"../../our_data/Etap5/GB_to_download.csv\", index=False)\n",
    "# US_names.to_csv(\"../../our_data/Etap5/US_to_download.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions communicating YT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchByQuery(query):\n",
    "\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=1,\n",
    "        q=query\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response\n",
    "\n",
    "def searchChannelIdByName(channel_name):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "#     request = youtube.search().list(part=\"snippet\", type=\"channel\", q=channel_name)\n",
    "    \n",
    "    request = youtube.channels().list(\n",
    "        part=\"id\",\n",
    "        forUsername=channel_name\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "#     print(response)\n",
    "    if response['pageInfo']['totalResults']==0:\n",
    "        print(\"NO channel ID for: \", channel_name)\n",
    "        return -1\n",
    "    return response['items'][0]['id']\n",
    "\n",
    "\n",
    "def getVideosOfChannelBetweenDates(channel_name, start_datetime, end_datetime):\n",
    "\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "    cID = searchChannelIdByName(channel_name)\n",
    "    if cID == -1:\n",
    "        return -1\n",
    "    request = youtube.search().list(\n",
    "        part=[\"snippet\"],\n",
    "        channelId =cID,\n",
    "        \n",
    "#         type='video',\n",
    "        publishedBefore=end_datetime,\n",
    "        publishedAfter=start_datetime,\n",
    "#         pageToken=\"CDIQAA\",\n",
    "        maxResults=NUMBER_OF_VIDEOS\n",
    "\n",
    "    )\n",
    "    \n",
    "    return request.execute()\n",
    "\n",
    "def getStatisticsForVideoID(videoID):\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "    \n",
    "    request = youtube.videos().list(\n",
    "        part = 'statistics',\n",
    "        maxResults = 1,\n",
    "        id = videoID\n",
    "    )\n",
    "    return request.execute()\n",
    "    \n",
    "# def getFeaturesForChannel(channel_name,start_datetime,end_datetime):\n",
    "    \n",
    "#     response_videos = getVideosOfChannelBetweenDates(channel_name, start_datetime, end_datetime)\n",
    "# #     print(response_videos)\n",
    "#     newDf = pd.DataFrame(columns=COLUMNS)\n",
    "#     if response_videos == -1:\n",
    "#         return newDf\n",
    "#     if response_videos['items'] == []:\n",
    "#         print(\"NO MOVIES\")\n",
    "#         return newDf\n",
    "    \n",
    "#     for vid in response_videos['items']:\n",
    "#         vid_id = vid['id']['videoId']\n",
    "#         stats = getStatisticsForVideoID(vid_id)\n",
    "#         data ={\n",
    "#         'video_id' : [vid_id],\n",
    "#         'trending_date' :[None],\n",
    "#         'title' : [vid['snippet']['title']],\n",
    "#         'channel_title' : [channel_name],\n",
    "#         'category_id': [stats['items'][0]['snippet']['categoryId']], #vid['snippet']['title']\n",
    "#         'publish_time' : [vid['snippet']['publishedAt']],\n",
    "#         'tags' : [stats['items'][0]['snippet']['tags']], #vid['snippet']['tags']\n",
    "#         'views' : [stats['items'][0]['statistics']['viewCount']],\n",
    "#         'likes' : [stats['items'][0]['statistics']['likeCount']],\n",
    "#         'dislikes' : [stats['items'][0]['statistics']['dislikeCount']],\n",
    "#         'comment_count' : [stats['items'][0]['statistics']['commentCount']],\n",
    "#         'thumbnail_link' : [vid['snippet']['thumbnails']['high']['url']],\n",
    "#         'comments_disabled': [None],\n",
    "#         'ratings_disabled': [None],\n",
    "#         'video_error_or_removed' : [None],\n",
    "#         'description ' : [vid['snippet']['description']]\n",
    "#         }\n",
    "#         temp = pd.DataFrame(data)\n",
    "#         newDf = pd.concat([newDf, temp], axis=0)\n",
    "#     return newDf\n",
    "\n",
    "\n",
    "def getNewVideosIDsForChannel(channel_name, start_datetime, end_datetime):\n",
    "    response_videos = getVideosOfChannelBetweenDates(channel_name, start_datetime, end_datetime)\n",
    "    newDf = pd.DataFrame(columns=COLUMNS)\n",
    "    if response_videos == -1:\n",
    "        return newDf\n",
    "    if response_videos['items'] == []:\n",
    "        print(\"NO MOVIES\")\n",
    "        return newDf\n",
    "    \n",
    "    for vid in response_videos['items']:\n",
    "        vid_id = vid['id']['videoId']\n",
    "        data = {\n",
    "            'video_id' : vid_id,\n",
    "            'title' : vid['snippet']['title'],\n",
    "            'channel_title' : channel_name,\n",
    "            'publish_time' : vid['snippet']['publishedAt'],\n",
    "            'description ' : vid['snippet']['description']\n",
    "        } \n",
    "        newDf = newDf.append(pd.Series(data=data), ignore_index=True)\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWNLOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trixie Mattel\n",
      "NO channel ID for:  Trixie Mattel\n",
      "MILO\n",
      "NO MOVIES\n",
      "AxwellIngrossoVEVO\n",
      "ERROR:  'videoId'\n",
      "Anna Akana\n",
      "NO channel ID for:  Anna Akana\n",
      "Vogue\n",
      "HOLLAND\n",
      "NO channel ID for:  HOLLAND\n",
      "Bravo\n",
      "NO MOVIES\n",
      "Asian Boss\n",
      "NO channel ID for:  Asian Boss\n",
      "LastWeekTonight\n",
      "PaleWavesVEVO\n",
      "NO channel ID for:  PaleWavesVEVO\n",
      "Voxis Productions\n",
      "NO channel ID for:  Voxis Productions\n",
      "FAST ESCAPE\n"
     ]
    }
   ],
   "source": [
    "GB_names_to_download =  pd.read_csv(\"../../our_data/Etap5/GB_to_download.csv\")\n",
    "US_names_to_download =  pd.read_csv(\"../../our_data/Etap5/US_to_download.csv\")\n",
    "\n",
    "names_to_delete = []\n",
    "names = pd.DataFrame()\n",
    "\n",
    "if download == 'GB':\n",
    "    names = GB_names_to_download\n",
    "elif download == \"US\":\n",
    "    names = US_names_to_download\n",
    "\n",
    "\n",
    "newDataDf = pd.DataFrame(columns=COLUMNS)\n",
    "    \n",
    "for i,row in names.iterrows():\n",
    "    name = row[0]\n",
    "    print(name)\n",
    "    try:\n",
    "        tempDF = getNewVideosIDsForChannel(name, start_datetime, end_datetime)\n",
    "        newDataDf = pd.concat([newDataDf, tempDF], axis=0)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        if exc_type == googleapiclient.errors.HttpError:\n",
    "            print(\"YT API LIMIT ACHIEVED\")\n",
    "            break\n",
    "        print(\"ERROR: \", e)\n",
    "    names_to_delete.append(name)\n",
    "    if i == 10:\n",
    "        print(\"FAST ESCAPE\")\n",
    "        break\n",
    "        \n",
    "# removing names\n",
    "new_names = set(names['0'])\n",
    "new_names = new_names.difference(set(names_to_delete))\n",
    "\n",
    "\n",
    "if download == 'GB':\n",
    "    GB_names = pd.DataFrame(new_names)\n",
    "    GB_names.to_csv(\"../../our_data/Etap5/GB_to_download.csv\", index=False)\n",
    "    \n",
    "    if os.path.isfile(\"../../our_data/Etap5/GB_new_data.csv\"):\n",
    "        oldDataDF = pd.read_csv(\"../../our_data/Etap5/GB_new_data.csv\")\n",
    "        newDataDf = pd.concat([newDataDf,oldDataDF], axis=0)\n",
    "    newDataDf.to_csv(\"../../our_data/Etap5/GB_new_data.csv\", index=False)\n",
    "    \n",
    "elif download == \"US\":\n",
    "    US_names = pd.DataFrame(new_names)\n",
    "    US_names.to_csv(\"../../our_data/Etap5/US_to_download.csv\", index=False)\n",
    "    \n",
    "    if os.path.isfile(\"../../our_data/Etap5/US_new_data.csv\"):\n",
    "        oldDataDF = pd.read_csv(\"../../our_data/Etap5/US_new_data.csv\")\n",
    "        newDataDf = pd.concat([newDataDf,oldDataDF], axis=0)\n",
    "        \n",
    "    newDataDf.to_csv(\"../../our_data/Etap5/US_new_data.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relatedToVideoId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiusxyygqGk\n",
    "def searchRelatedVideosForVideoID(video_id, maxResults=50):\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # range of time probably does not work\n",
    "    start_datetime = \"2017-11-14T23:59:59Z\"\n",
    "    end_datetime = \"2018-06-14T23:59:59Z\"\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=[\"snippet\"],\n",
    "        relatedToVideoId=video_id,\n",
    "        type='video',\n",
    "        publishedBefore=end_datetime,\n",
    "        publishedAfter=start_datetime,\n",
    "        maxResults=maxResults\n",
    "    )\n",
    "\n",
    "    response = request.execute()\n",
    "    return response\n",
    "\n",
    "# res = searchRelatedVideosForVideoID(\"fiusxyygqGk\", maxResults=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YT API LIMIT ACHIEVED\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "\n",
    "list_of_ids_path = \"../../our_data/Etap5/related_list_of_ids.csv\"\n",
    "if os.path.isfile(list_of_ids_path):\n",
    "    with open(list_of_ids_path) as file:\n",
    "        list_of_ids = json.loads(file.read())\n",
    "else:\n",
    "    df = pd.read_csv('../../our_data/Etap4/GB_US-rdy-to-learn-properly-V2.csv')\n",
    "    list_of_ids = df.video_id.to_list()\n",
    "\n",
    "new_videos = pd.DataFrame(columns=COLUMNS + [\"relatedTo\"])\n",
    "for i, video_id in enumerate(list_of_ids):\n",
    "    try:\n",
    "        response = searchRelatedVideosForVideoID(video_id, maxResults=50)\n",
    "    except Exception as e:\n",
    "        with open(\"../../our_data/Etap5/related_list_of_ids.csv\", 'w') as file:\n",
    "            file.write(json.dumps(list_of_ids))\n",
    "            \n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        if exc_type == googleapiclient.errors.HttpError:\n",
    "            print(\"YT API LIMIT ACHIEVED\")\n",
    "            break\n",
    "        print(\"ERROR: \", e)\n",
    "                                                 \n",
    "                                             \n",
    "    for vid in response['items']:\n",
    "        if 'snippet' not in vid:\n",
    "            continue\n",
    "        data = {\n",
    "            'video_id' : vid['id']['videoId'],\n",
    "            'relatedTo': video_id,\n",
    "            'title' : vid['snippet']['title'],\n",
    "            'channel_title' : vid['snippet']['channelTitle'],\n",
    "            'publish_time' : vid['snippet']['publishedAt'],\n",
    "            'description ' : vid['snippet']['description']\n",
    "        } \n",
    "        new_videos = new_videos.append(pd.Series(data=data), ignore_index=True)\n",
    "                                             \n",
    "    list_of_ids.remove(video_id)\n",
    "    \n",
    "    if i == 2:\n",
    "        print(f\"A kończymy sobie tutaj po {i} iteracjach.\")\n",
    "        break\n",
    "\n",
    "# Zapisywanie pozostałych video_id.\n",
    "with open(\"../../our_data/Etap5/related_list_of_ids.csv\", 'w') as file:\n",
    "    file.write(json.dumps(list_of_ids))\n",
    "    \n",
    "save_path = \"../../our_data/Etap5/relatedToVideoId.csv\"\n",
    "if os.path.isfile(save_path):\n",
    "    new_videos.to_csv(save_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    new_videos.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
