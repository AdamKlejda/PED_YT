{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from utils import searchVideosByListOfIds, fillCategoryIds\n",
    "from datetime import datetime, timedelta\n",
    "import sys, traceback\n",
    "\n",
    "# api_key = os.environ.get('YT_API')\n",
    "api_key = \"XYZ\"\n",
    "\n",
    "# df = pd.read_csv(\"../../our_data/Etap4/GB_US-rdy-to-learn-properly-V2.csv\")\n",
    "# df = pd.read_csv(\"../../our_data/Etap3/dfGB_merged.csv\")\n",
    "# df = pd.read_csv(\"../../our_data/Etap1/GB_US.csv\")\n",
    "\n",
    "todownload = {0:\"US\",1:\"GB\"}\n",
    "download = todownload[0] #HERE CHOSE WHICH ONE TO DOWNLOAD\n",
    "NUMBER_OF_VIDEOS = 10\n",
    "\n",
    "start_datetime = \"2017-11-14T23:59:59Z\"\n",
    "end_datetime = \"2018-06-14T23:59:59Z\"\n",
    "\n",
    "\n",
    "COLUMNS =['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
    "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
    "       'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
    "       'video_error_or_removed', 'description ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING LIST OF ACC NAMES (RUN IT IF YOU NEED A NEW ONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBdf = pd.read_csv(\"./../../data/youtube_data/GB_videos_5p.csv\", sep=';', encoding='utf_16')\n",
    "# USdf = pd.read_csv(\"./../../data/youtube_data/US_videos_5p.csv\", sep=';')\n",
    "# GB_names = pd.DataFrame(set(GBdf.channel_title))\n",
    "# US_names = pd.DataFrame(set(USdf.channel_title))\n",
    "# GB_names.to_csv(\"../../our_data/Etap5/GB_to_download.csv\", index=False)\n",
    "# US_names.to_csv(\"../../our_data/Etap5/US_to_download.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions communicating YT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchByQuery(query):\n",
    "\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=1,\n",
    "        q=query\n",
    "    )\n",
    "    response = request.execute()\n",
    "    return response\n",
    "\n",
    "def searchChannelIdByName(channel_name):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"id\",\n",
    "        forUsername=channel_name\n",
    "    )\n",
    "    response = request.execute()\n",
    "    print(response)\n",
    "    if response['pageInfo']['totalResults']==0:\n",
    "        print(\"NO channel ID\")\n",
    "        return -1\n",
    "    return response['items'][0]['id']\n",
    "\n",
    "\n",
    "def getVideosOfChannelBetweenDates(channel_name, start_datetime, end_datetime):\n",
    "\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "    cID = searchChannelIdByName(channel_name)\n",
    "    if cID == -1:\n",
    "        return -1\n",
    "    request = youtube.search().list(\n",
    "        part=[\"snippet\"],\n",
    "        channelId =cID,\n",
    "        \n",
    "#         type='video',\n",
    "        publishedBefore=end_datetime,\n",
    "        publishedAfter=start_datetime,\n",
    "#         pageToken=\"CDIQAA\",\n",
    "        maxResults=NUMBER_OF_VIDEOS\n",
    "\n",
    "    )\n",
    "    \n",
    "    return request.execute()\n",
    "\n",
    "def getStatisticsForVideoID(videoID):\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        \"youtube\", \"v3\", developerKey=api_key)\n",
    "    \n",
    "    request = youtube.videos().list(\n",
    "    part= 'statistics',\n",
    "    maxResults=1,\n",
    "    id = videoID\n",
    "    )\n",
    "    return request.execute()\n",
    "    \n",
    "def getFeaturesForChannel(channel_name,start_datetime,end_datetime):\n",
    "    \n",
    "    response_videos = getVideosOfChannelBetweenDates(channel_name, start_datetime, end_datetime)\n",
    "#     print(response_videos)\n",
    "    newDf = pd.DataFrame(columns=COLUMNS)\n",
    "    if response_videos == -1:\n",
    "        return newDf\n",
    "    if response_videos['items'] == []:\n",
    "        print(\"NO MOVIES\")\n",
    "        return newDf\n",
    "    \n",
    "    for vid in response_videos['items']:\n",
    "        vid_id = vid['id']['videoId']\n",
    "        stats = getStatisticsForVideoID(vid_id)\n",
    "        data ={\n",
    "        'video_id' : [vid_id],\n",
    "        'trending_date' :[None],\n",
    "        'title' : [vid['snippet']['title']],\n",
    "        'channel_title' : [channel_name],\n",
    "        'category_id': [None], #vid['snippet']['title']\n",
    "        'publish_time' : [vid['snippet']['publishedAt']],\n",
    "        'tags' : [None], #vid['snippet']['tags']\n",
    "        'views' : [stats['items'][0]['statistics']['viewCount']],\n",
    "        'likes' : [stats['items'][0]['statistics']['likeCount']],\n",
    "        'dislikes' : [stats['items'][0]['statistics']['dislikeCount']],\n",
    "        'comment_count' : [stats['items'][0]['statistics']['commentCount']],\n",
    "        'thumbnail_link' : [vid['snippet']['thumbnails']['high']['url']],\n",
    "        'comments_disabled': [None],\n",
    "        'ratings_disabled': [None],\n",
    "        'video_error_or_removed' : [None],\n",
    "        'description ' : [vid['snippet']['description']]\n",
    "        }\n",
    "        temp = pd.DataFrame(data)\n",
    "        newDf = pd.concat([newDf, temp], axis=0)\n",
    "    return newDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWNLOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nicki Minaj\n",
      "YT API LIMIT ACHIVED\n"
     ]
    }
   ],
   "source": [
    "GB_names_to_download =  pd.read_csv(\"../../our_data/Etap5/GB_to_download.csv\")\n",
    "US_names_to_download =  pd.read_csv(\"../../our_data/Etap5/US_to_download.csv\")\n",
    "\n",
    "names_to_delete = []\n",
    "names = pd.DataFrame()\n",
    "\n",
    "if download == 'GB':\n",
    "    names = GB_names_to_download\n",
    "elif download == \"US\":\n",
    "    names = US_names_to_download\n",
    "\n",
    "\n",
    "newDataDf = pd.DataFrame(columns=COLUMNS)\n",
    "    \n",
    "for i,row in names.iterrows():\n",
    "    name = row[0]\n",
    "    print(name)\n",
    "    try:\n",
    "        tempDF = getFeaturesForChannel(name,start_datetime,end_datetime)\n",
    "        newDataDf = pd.concat([newDataDf, tempDF], axis=0)\n",
    "    except Exception as e:\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        if exc_type == googleapiclient.errors.HttpError:\n",
    "            print(\"YT API LIMIT ACHIVED\")\n",
    "            break\n",
    "        print(\"ERROR: \", e)\n",
    "    names_to_delete.append(name)\n",
    "        \n",
    "        \n",
    "# removing names\n",
    "new_names = set(names['0'])\n",
    "new_names = new_names.difference(set(names_to_delete))\n",
    "\n",
    "\n",
    "if download == 'GB':\n",
    "    GB_names = pd.DataFrame(new_names)\n",
    "    GB_names.to_csv(\"../../our_data/Etap5/GB_to_download.csv\", index=False)\n",
    "    \n",
    "    if os.path.isfile(\"../../our_data/Etap5/GB_new_data.csv\"):\n",
    "        oldDataDF = pd.read_csv(\"../../our_data/Etap5/GB_new_data.csv\")\n",
    "        newDataDf = pd.concat([newDataDf,oldDataDF], axis=0)\n",
    "    newDataDf.to_csv(\"../../our_data/Etap5/GB_new_data.csv\")\n",
    "    \n",
    "elif download == \"US\":\n",
    "    US_names = pd.DataFrame(new_names)\n",
    "    US_names.to_csv(\"../../our_data/Etap5/US_to_download.csv\", index=False)\n",
    "    \n",
    "    if os.path.isfile(\"../../our_data/Etap5/US_new_data.csv\"):\n",
    "        oldDataDF = pd.read_csv(\"../../our_data/Etap5/US_new_data.csv\")\n",
    "        newDataDf = pd.concat([newDataDf,oldDataDF], axis=0)\n",
    "        \n",
    "    newDataDf.to_csv(\"../../our_data/Etap5/US_new_data.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
